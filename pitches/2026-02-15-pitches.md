# Content Mining Session: 2026-02-15

**Mined by:** content-miner agent
**Sources scanned:** 5 journal entries (Feb 10-14), 1 brain report, 3 ideas docs, 2 project docs, 1 interactive risk session analysis, 5 existing drafts, 4 published posts
**Pitches produced:** 4

---

## Pitch: Why Your AI Pipeline's Performance Is a Coin Flip (And How to Fix It)

**Lane:** // build
**Format:** Blog post
**Register:** Blended (Register 1 hook → Register 2 analysis)

### Hook
Two identical runs of the same AI pipeline, same inputs, same model. One took 19 minutes. The other took 29 minutes with 48% more API calls. The cause wasn't infrastructure. It was a single configuration parameter most teams never touch.

### Arc
Open with the mystery: two back-to-back runs producing wildly different performance profiles despite identical inputs. Walk through the forensic investigation — isolating variance to the review phase while discovery, research, analysis, and compliance showed zero variance. Reveal the culprit: LLM temperature at 0.3 in critique agents, where a single threshold difference in one agent's response cascades into additional review cycles, extra context, and compounding delays. Present the fix: differentiated temperature settings by cognitive task type (deterministic evaluation at 0.1, creative exploration at 0.3). Close with the transferable principle: in multi-agent systems, the agents doing evaluation work need different tuning than the agents doing creative work, and the industry consensus (OpenAI, Anthropic, Google) is temperature below 0.1 for production evaluation tasks.

### Key Data Points
- Run 1: 1,151s (19.2 min), 23 agent calls
- Run 2: 1,754s (29.2 min), 34 agent calls — identical inputs
- Variance: +52% duration, +64% in one agent type, +80% in another
- All non-review phases: zero variance
- Root cause: one critique phrased as "missing evidence" (fast rejection) vs. "needs specific metrics X, Y, Z" (triggers revision loop → +10 min cascade)
- Industry guidance: temperature below 0.1 for evaluation tasks
- Expected fix impact: 84% variance reduced to 15-20% (76% reduction)

### Source Material
- `~/journal/entries/2026-02-11.md` — PRODSECRM-105 variance investigation section with full evidence breakdown
- `~/journal/projects/prodsecrm-risk-assessment.md` — Variance investigation summary with solution design
- `~/journal/ideas/model-selection-optimization-strategy.md` — Flow engineering principles and model selection by cognitive load

### Sensitivity Flags
- Must not name the specific pipeline, platform, or risk identifiers
- Frame as "a multi-agent risk assessment pipeline" or "a production AI workflow"
- Do not reference specific cloud providers or model versions that could identify employer
- Temperature values and variance percentages are generic/public — safe to use

### Why Now
Every team deploying multi-agent AI systems in production is discovering non-determinism the hard way. Most assume it's infrastructure. Almost nobody is tuning temperature by agent role. This gives practitioners a concrete, reproducible fix.

---

## Pitch: Flow Engineering: Teaching AI Without Training It

**Lane:** // build
**Format:** Blog post
**Register:** Blended (Register 1 walkthrough → Register 2 principles)

### Hook
I run a pipeline with 10 AI agents. I've never fine-tuned a single one. Every behavior change — from how strictly they evaluate, to when they stop debating — is controlled by graph logic and prompt architecture, not model weights.

### Arc
Start with the counterintuitive claim: you don't need to train AI models to get production-quality behavior. Walk through the three levers of flow engineering: (1) prompt architecture using structured Speech Acts (CRITICIZE, ENDORSE, REFINE) that force specific cognitive stances, (2) graph logic with convergence scoring that determines when agents stop iterating, and (3) model tiering that assigns expensive reasoning models to complex tasks and cheap fast models to pattern-matching tasks. Show real numbers: a compliance classification agent moved from a reasoning model to a fast model with zero quality degradation and 50% latency reduction. Close with the principle: in multi-agent systems, the architecture IS the training. You shape behavior through graph topology and prompt design, not through weight updates — and this is more maintainable, auditable, and adaptable than fine-tuning.

### Key Data Points
- 10 specialized agents, zero fine-tuning
- Convergence threshold: 0.85 (measurable termination criteria)
- Model tiering: compliance classification on fast model — 10s vs 20s (50% reduction), zero quality degradation
- Performance optimization: 956s baseline → 697s optimized (-27%) through graph logic changes, not model changes
- Cost projection: 60-70% inference cost reduction through cognitive load-based model routing
- 601 tests passing, 89% coverage — the graph logic is testable in ways model weights aren't

### Source Material
- `~/journal/ideas/model-selection-optimization-strategy.md` — Core flow engineering principles, model selection by cognitive load
- `~/journal/ideas/phase-5-enterprise-bridge-cognitive-refinement.md` — Speech Acts implementation design
- `~/journal/entries/2026-02-11.md` — Performance optimization results (Phase A-D), model tiering validation
- `~/journal/entries/2026-02-10.md` — Baseline measurements and context tracking

### Sensitivity Flags
- Must not name the specific risk assessment domain or employer
- Frame as "a multi-agent security analysis pipeline" generically
- Do not reference specific cloud APIs or enterprise integrations
- Speech Acts, convergence scoring, and model tiering are generic architectural patterns — safe to discuss
- Do not mention specific model names by vendor (use "reasoning model" and "fast model")

### Why Now
The AI industry is fixated on fine-tuning and training as the path to better outputs. Flow engineering is the alternative that most practitioners haven't considered — cheaper, faster to iterate, fully auditable, and testable with standard software engineering practices. As multi-agent architectures become mainstream (Anthropic's MCP, OpenAI's agents SDK), this framing gives practitioners a concrete alternative to the "just fine-tune it" reflex.

---

## Pitch: I Built the Scanner Because the Data Didn't Exist

**Lane:** // defense
**Format:** LinkedIn post
**Register:** Blended (Register 1 narrative → Register 2 pattern)

### Hook
A critical-severity security risk had been on the register for over a year. The risk assessment said "no internal metrics on content scanning coverage." So I built the scanner myself — and what I found halved the resource ask.

### Arc
Open with the problem: a security risk that couldn't be quantified because the data didn't exist. Nobody was negligent — the platform simply lacked detection capabilities. Instead of waiting for another team to build it, take the engineer's approach: build a proof-of-concept scanner. The scanner transforms an unmeasurable concern into a quantified risk with specific numbers. Then show the strategic leverage: with detection solved, the remaining work (enforcement) is half the original scope — changing a 4-6 FTE-week resource request into a 2-3 FTE-week ask. Close with the principle: in risk management, if you can't measure it, you can't scope the fix. Sometimes the fastest way to move a stalled risk forward is to build the measurement tool yourself.

### Key Data Points
- Risk age: 22+ months on the register before quantification
- Scanner results: 13,931 repos scanned, 327 flagged
- Detection accuracy: 93.4% precision, 100% recall
- Resource impact: original ask 4-6 FTE weeks → revised ask 2-3 FTE weeks (detection already built)
- False positive reduction: 83% fewer false positives using probabilistic scoring vs. additive heuristics
- Compliance gap conversion: multiple frameworks moved from "no control exists" to "detective control in place"

### Source Material
- `~/interactive-risk-sessions/PRODSECRM-40/analysis.md` — Full analysis with scanner capability mapping, resource impact, compliance gaps
- `~/journal/entries/2026-02-14.md` — JIRA comment draft and stakeholder email with refined resource ask
- `~/journal/entries/2026-02-10.md` — Scanner demo context and meeting outcomes

### Sensitivity Flags
- Must not name the platform, registry, employer, or team members
- Frame as "a public container registry" or "a content hosting platform"
- Do not reference specific compliance frameworks (SOC2, FedRAMP, CRA) by name — use "industry compliance standards"
- Do not mention JIRA ticket numbers, organizational hierarchy, or internal escalation paths
- Scanner architecture details (heuristic rules, threat intel integration) are safe to discuss generically

### Why Now
Container security is a top concern as registries scale. The "build the measurement tool" pattern is transferable to any risk that's stuck because nobody can quantify it. Security engineers who build tooling but can't get organizational traction will recognize this situation immediately.

---

## Pitch: The 7-Layer Security Model for Self-Improving AI Agents

**Lane:** // build
**Format:** Blog post
**Register:** 2 (analytical/authority)

### Hook
I designed a system where AI agents automatically improve their own prompts based on production performance data. The first thing I had to build wasn't the improvement engine — it was 7 layers of security to prevent the system from poisoning itself.

### Arc
Open with the design goal: a feedback loop where production failures generate new test cases, test cases expose agent weaknesses, and the system proposes prompt improvements — automatically. Then reveal the core tension: any system that modifies its own instructions is a prompt injection target. Walk through the 7-layer security model: (1) SHA-256 content hash validation, (2) role-based approval gating, (3) deterministic failure classification — never let an LLM decide what to auto-fix, (4) HMAC-SHA256 chain verification on production data, (5) dry-run by default with explicit --apply flag, (6) quality baseline tracking with automatic rollback, (7) full audit trail. Highlight the critical insight: the most dangerous vulnerability was Goodhart's Law — the system optimizing for eval metrics rather than actual quality. The mitigation: human review gates at every mutation point, and a strict rule that LLM judge failures always require human review (no LLM-based classification of what went wrong). Close with the principle: self-improving AI systems require more security engineering than traditional software, but the patterns (defense-in-depth, fail-safe defaults, deterministic validation) are well-established and transferable.

### Key Data Points
- 7 security layers, each addressing a specific attack vector
- Critical vulnerability found: prompt injection via eval reports (eval reports contain user-originated data from issue trackers)
- Critical vulnerability found: Goodhart's Law — optimizing for metrics degrades actual quality
- Dual feedback loop: development-time (CI evals) + production-informed (weekly metrics analysis)
- Dry-run default: --apply flag required for any mutation (CI is always read-only)
- PII leakage vector identified: production data flows through 6 transformation stages before potentially reaching version control
- Plan size: 1,490 lines for the self-improvement system + 2,120 lines for the production feedback loop — both passed security red team review on first revision

### Source Material
- `~/journal/entries/2026-02-13.md` — Full 7-layer security model, dual feedback loops, key learnings, vulnerability analysis
- `~/journal/ideas/model-selection-optimization-strategy.md` — Flow engineering context (adversarial testing section)

### Sensitivity Flags
- Must not name employer, pipeline, or specific risk domain
- Frame as "a multi-agent AI pipeline for security analysis" generically
- Do not reference specific CI/CD platforms (GitLab), issue trackers (JIRA), or cloud providers
- Do not mention specific model vendors in the context of eval judge implementation
- The 7-layer security model, Goodhart's Law mitigation, and deterministic classification patterns are generic — safe to discuss in detail
- Do not reference specific regulatory frameworks or compliance requirements that could identify the employer

### Why Now
Self-improving AI agents are the next frontier — but the security implications are barely discussed. Most teams building "auto-prompt-tuning" or "self-healing" AI systems haven't thought through the attack surface. This gives practitioners a concrete security architecture before they build something that optimizes itself into a vulnerability.

---

## Pitch: From Gemini Gems to 10 Autonomous Agents in 8 Weeks

**Lane:** // build
**Format:** Blog post (flagship piece — this is the origin story)
**Register:** Blended (Register 1 narrative throughout, Register 2 for technical lessons)

### Hook
Eight weeks ago, I typed a risk assessment question into Gemini and got back a response that classified routine technical debt as a critical systemic failure. Today I run a pipeline of 10 specialized AI agents with 1,189 tests, 90% coverage, and a red team that argues with its own findings until they're defensible. Here's every failure that got me from there to here.

### Arc
Tell the story as a six-phase evolution, each phase triggered by a specific failure in the previous one. Phase I: vanilla Gemini produces risk amplification bias — LLMs without encoded risk appetite miscategorize everything as critical. Phase II: build a custom model layer ("Project Solomon") with RAG, authority hierarchies, and weighted data tiers — hit the knowledge base ceiling, discover instruction drift where advisory data overrides governance protocols, encounter "silent context loss" where the model fakes knowledge after context window saturation. Phase III: transition to Claude Code and agentic architecture — use AI to decompose the monolithic system into modular agents (the recursive development model). Phase IV: production integration with JIRA middleware. Phase V: workspace isolation, stateful execution, adversarial red teaming. Phase VI: the current system with convergence scoring, compliance mapping, and self-improving agents.

The through-line: each phase exists because the previous one broke in a specific, instructive way. The "learn by doing" philosophy applied to AI system design. The key insight at the end: the most effective enterprise AI implementation is not the automation of results, but the automation of the auditing process.

### Key Data Points
- Phase I: zero-shot prompting → risk amplification bias (routine debt classified as critical)
- Phase II: 8-file knowledge base limit → instruction drift, advisory overriding governance
- Phase II: context window saturation → "silent context loss" (model outputs high-confidence responses about context it has already purged)
- Phase II: 3-tier authority weighting (Governance 1.0, JIRA 0.7, User Input 0.3)
- Phase III: monolithic → modular decomposition → ~40% reduction in hallucination rates
- Phase III → today: 10 specialized agents, 1,189 tests, 90% coverage
- Performance: 27% speed improvement through graph logic changes alone
- Variance: 84% → 15-20% through differentiated temperature tuning
- Timeline: ~8 weeks from first Gemini prompt to production multi-agent pipeline

### Source Material
- Gemini Canvas retrospective document (user-provided, Jan 27 2026) — the full six-phase arc with technical details
- `~/journal/entries/2026-02-10.md` — Phase 0 infrastructure fixes, baseline measurements
- `~/journal/entries/2026-02-11.md` — Performance optimization, variance investigation
- `~/journal/entries/2026-02-13.md` — Self-improvement system, dual feedback loops
- `~/journal/projects/prodsecrm-risk-assessment.md` — System architecture overview
- `~/journal/ideas/model-selection-optimization-strategy.md` — Flow engineering principles

### Sensitivity Flags
- Must not name "Project Solomon" as it's an internal project name — use "a custom model layer" or "a RAG-based system"
- Must not reference JIRA by name in the context of employer workflows — use "issue tracker" or "system of record"
- Must not name the employer, team, or specific risk domain
- Google Forms → JIRA integration details are too specific — generalize to "automated intake pipeline"
- Gemini and Claude can be named (they're public products Ian used personally)
- The technical patterns (authority weighting, context saturation, modular decomposition) are generic and safe
- "Silent context loss" is a universal LLM phenomenon — safe to discuss

### Why Now
Everyone is experimenting with AI agents but most are still in Phase I or II — getting inconsistent results from monolithic prompts and wondering why. This piece gives them the full roadmap of what breaks and what to build next, from someone who went through every phase in real time. The "silent context loss" finding alone (LLMs confidently generating responses about context they've already dropped) is something every enterprise AI team needs to hear.

---

## Mining Notes

### Signals Considered but Not Pitched

**The FTE Estimate That Didn't Survive a Spreadsheet (PRODSECRM-176)**
- Strong signal: 2 FTE estimate challenged down to 0.375 FTE with industry benchmarks
- NOT pitched because: this exact pitch appears as the calibration example in the agent instructions. Ian has already seen it framed this way. If he wants it, it's ready — but pitching it again would be redundant.

**Premature Risk Closure / "Mitigated" Without Verification**
- Strong signal from PRODSECRM-40 analysis (Feb 2025: "risk successfully mitigated" → immediate re-population)
- NOT pitched because: this pattern is already covered in the drafted Commitment-Without-Execution Loop post. The specific "premature closure" dynamic is step 3 of that framework.

**Measure First, Optimize Second (474K task parameter discovery)**
- Interesting signal from Feb 10: assumed context parameter was bloated, measured and found task parameter was the real problem
- NOT pitched because: the data point is strong but the narrative is too narrow for a standalone piece. Better as a supporting example in the Flow Engineering pitch or the temperature variance pitch.

### Coverage Assessment

| Lane | Pitches | Notes |
|------|---------|-------|
| // offense | 0 | No strong non-duplicate signals in the 14-day window. HTB work not in recent journals. |
| // defense | 1 | "I Built the Scanner" covers risk-as-engineering territory |
| // build | 3 | Heavy build signal from Feb 10-14 engineering work — temperature tuning, flow engineering, self-improvement security |

The build lane is overweighted because the last 14 days were heavily engineering-focused. When Ian resumes offense work (HTB, CTFs), those signals will surface. The defense lane has one strong pitch that maps directly to Ian's core territory (risk-as-engineering).
